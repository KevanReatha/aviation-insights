{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8378d9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /Users/kevantamom/Desktop/GITHUB/aviation-insights/notebooks\n",
      "RAW exists: True\n",
      "File exists: True\n",
      "File size (bytes): 1432318\n",
      "\n",
      "RAW folder CSVs:\n",
      " - International_airline_activity_0525_Table4.csv\n",
      " - International_airline_activity_0525_Table5.csv\n",
      " - International_airline_activity_0525_Table2.csv\n",
      " - International_airline_activity_0525_Table3.csv\n",
      " - International_airline_activity_0525_Table1.csv\n",
      " - International_airline_activity_Table1_2009toCurrent_0525_Data - Sheet1.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RAW = Path(\"../data/raw\")  # adjust if your notebook lives elsewhere\n",
    "p = RAW / \"International_airline_activity_Table1_2009toCurrent_0525_Data - Sheet1.csv\"\n",
    "\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"RAW exists:\", RAW.exists())\n",
    "print(\"File exists:\", p.exists())\n",
    "if p.exists():\n",
    "    print(\"File size (bytes):\", p.stat().st_size)\n",
    "\n",
    "print(\"\\nRAW folder CSVs:\")\n",
    "for f in RAW.glob(\"*.csv\"):\n",
    "    print(\" -\", f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c9e95",
   "metadata": {},
   "source": [
    "# t1Hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab4f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def _month_from_text(m):\n",
    "    # Accept \"Jan-09\", \"January 2009\", \"Jan\", 1..12\n",
    "    s = str(m).strip()\n",
    "    try:\n",
    "        v = int(s)\n",
    "        return max(1, min(12, v))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        return datetime.strptime(s[:3], \"%b\").month\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def _to_number(x: pd.Series) -> pd.Series:\n",
    "    # Handle \"..\", blanks, thousands commas\n",
    "    return (\n",
    "        x.astype(str)\n",
    "         .str.strip()\n",
    "         .replace({\"..\": np.nan, \"—\": np.nan, \"-\": np.nan, \"\": np.nan})\n",
    "         .str.replace(\",\", \"\", regex=False)\n",
    "         .astype(float)\n",
    "    )\n",
    "\n",
    "def clean_table1_hist(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Standardise headers (keep original labels, just trim)\n",
    "    df = df_raw.rename(columns=lambda c: str(c).strip())\n",
    "\n",
    "    # Map your exact column names (with regex fallbacks if labels vary)\n",
    "    col_map = {\n",
    "        \"Month\":           [c for c in df.columns if re.fullmatch(r\"(?i)month\", c)],\n",
    "        \"Airline_Name\":    [c for c in df.columns if re.fullmatch(r\"(?i)(scheduled\\s+operator|airline|carrier)\", c)],\n",
    "        \"Country\":         [c for c in df.columns if re.fullmatch(r\"(?i)country\\s*to/?from\", c)],\n",
    "        \"Passengers In\":   [c for c in df.columns if re.fullmatch(r\"(?i)passengers?\\s*in\", c)],\n",
    "        \"Freight In\":      [c for c in df.columns if re.fullmatch(r\"(?i)freight\\s*in\", c)],\n",
    "        \"Mail In\":         [c for c in df.columns if re.fullmatch(r\"(?i)mail\\s*in\", c)],\n",
    "        \"Passengers Out\":  [c for c in df.columns if re.fullmatch(r\"(?i)passengers?\\s*out\", c)],\n",
    "        \"Freight Out\":     [c for c in df.columns if re.fullmatch(r\"(?i)freight\\s*out\", c)],\n",
    "        \"Mail Out\":        [c for c in df.columns if re.fullmatch(r\"(?i)mail\\s*out\", c)],\n",
    "        \"Year\":            [c for c in df.columns if re.fullmatch(r\"(?i)year\", c)],\n",
    "    }\n",
    "    pick = lambda k: col_map[k][0] if col_map[k] else None\n",
    "\n",
    "    m_col   = pick(\"Month\")\n",
    "    a_col   = pick(\"Airline_Name\")\n",
    "    c_col   = pick(\"Country\")\n",
    "    y_col   = pick(\"Year\")\n",
    "    pin_col = pick(\"Passengers In\")\n",
    "    fin_col = pick(\"Freight In\")\n",
    "    min_col = pick(\"Mail In\")\n",
    "    pout_col= pick(\"Passengers Out\")\n",
    "    fout_col= pick(\"Freight Out\")\n",
    "    mout_col= pick(\"Mail Out\")\n",
    "\n",
    "    # Keep only available columns\n",
    "    keep = [x for x in [m_col,a_col,c_col,y_col,pin_col,fin_col,min_col,pout_col,fout_col,mout_col] if x]\n",
    "    df = df[keep].copy()\n",
    "\n",
    "    # Rename to canonical\n",
    "    ren = {}\n",
    "    if a_col:   ren[a_col] = \"Airline_Name\"\n",
    "    if c_col:   ren[c_col] = \"Country\"\n",
    "    if y_col:   ren[y_col] = \"Year\"\n",
    "    if m_col:   ren[m_col] = \"MonthText\"\n",
    "    if pin_col: ren[pin_col] = \"Passengers_In\"\n",
    "    if fin_col: ren[fin_col] = \"Freight_In\"\n",
    "    if min_col: ren[min_col] = \"Mail_In\"\n",
    "    if pout_col:ren[pout_col] = \"Passengers_Out\"\n",
    "    if fout_col:ren[fout_col] = \"Freight_Out\"\n",
    "    if mout_col:ren[mout_col] = \"Mail_Out\"\n",
    "    df = df.rename(columns=ren)\n",
    "\n",
    "    # Parse Month\n",
    "    if \"MonthText\" in df.columns:\n",
    "        df[\"Month\"] = df[\"MonthText\"].apply(_month_from_text).astype(\"Int64\")\n",
    "    else:\n",
    "        df[\"Month\"] = pd.NA\n",
    "\n",
    "    # Numeric coercion\n",
    "    for col in [\"Passengers_In\",\"Freight_In\",\"Mail_In\",\"Passengers_Out\",\"Freight_Out\",\"Mail_Out\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = _to_number(df[col])\n",
    "\n",
    "    # Build long/tidy output\n",
    "    keys = [k for k in [\"Airline_Name\",\"Country\",\"Year\",\"Month\"] if k in df.columns]\n",
    "\n",
    "    inbound = df[keys].copy()\n",
    "    if \"Passengers_In\" in df: inbound[\"Passengers\"] = df[\"Passengers_In\"]\n",
    "    if \"Freight_In\"   in df: inbound[\"Freight_tonnes\"] = df[\"Freight_In\"]\n",
    "    if \"Mail_In\"      in df: inbound[\"Mail_tonnes\"]    = df[\"Mail_In\"]\n",
    "    inbound[\"Direction\"] = \"Inbound\"\n",
    "\n",
    "    outbound = df[keys].copy()\n",
    "    if \"Passengers_Out\" in df: outbound[\"Passengers\"] = df[\"Passengers_Out\"]\n",
    "    if \"Freight_Out\"   in df: outbound[\"Freight_tonnes\"] = df[\"Freight_Out\"]\n",
    "    if \"Mail_Out\"      in df: outbound[\"Mail_tonnes\"]    = df[\"Mail_Out\"]\n",
    "    outbound[\"Direction\"] = \"Outbound\"\n",
    "\n",
    "    out = pd.concat([inbound, outbound], ignore_index=True)\n",
    "\n",
    "    # Tag & clean \"ALL SERVICES\" (if ever present)\n",
    "    out[\"Is_Total_AllServices\"] = out.get(\"Airline_Name\", pd.Series(\"\", index=out.index)).astype(str)\\\n",
    "        .str.contains(\"ALL SERVICES\", case=False, na=False)\n",
    "    out[\"Airline_Name\"] = out.get(\"Airline_Name\", pd.Series(\"\", index=out.index)).astype(str)\\\n",
    "        .str.replace(r\"\\s*ALL SERVICES\\s*\", \"\", regex=True).str.strip()\n",
    "\n",
    "    # Build YearMonthDate\n",
    "    if \"Year\" in out.columns:\n",
    "        out[\"Month\"] = out[\"Month\"].fillna(1).astype(\"Int64\")\n",
    "        out[\"YearMonthDate\"] = pd.to_datetime(\n",
    "            dict(year=out[\"Year\"], month=out[\"Month\"].astype(\"int\"), day=1), errors=\"coerce\"\n",
    "        )\n",
    "    else:\n",
    "        out[\"YearMonthDate\"] = pd.NaT\n",
    "\n",
    "    # Final ordering\n",
    "    cols = [\"Year\",\"Month\",\"YearMonthDate\",\"Airline_Name\",\"Country\",\"Direction\",\n",
    "            \"Passengers\",\"Freight_tonnes\",\"Mail_tonnes\",\"Is_Total_AllServices\"]\n",
    "    out = out[[c for c in cols if c in out.columns]].sort_values(\n",
    "        [c for c in [\"Year\",\"Month\",\"Airline_Name\",\"Country\",\"Direction\"] if c in out.columns]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fafdf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hist_raw = pd.read_csv(\"../data/raw/International_airline_activity_Table1_2009toCurrent_0525_Data - Sheet1.csv\")\n",
    "t1_hist  = clean_table1_hist(hist_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68857e30",
   "metadata": {},
   "source": [
    "# table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f865f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def _to_number(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.astype(str)\n",
    "         .str.strip()\n",
    "         .replace({\"..\": np.nan, \"—\": np.nan, \"-\": np.nan, \"\": np.nan})\n",
    "         .str.replace(\",\", \"\", regex=False)\n",
    "         .astype(float)\n",
    "    )\n",
    "\n",
    "def clean_table1_may_from_sample(df_raw: pd.DataFrame, year=2025, month=5) -> pd.DataFrame:\n",
    "    # 1) Standardise headers (trim + underscores)\n",
    "    df = df_raw.rename(columns=lambda c: str(c).strip().replace(\" \", \"_\")).copy()\n",
    "\n",
    "    # 2) Rename key columns to canonical names\n",
    "    ren = {\n",
    "        \"Scheduled_Operator\": \"Airline_Name\",\n",
    "        \"Country_to/from\": \"Country\",\n",
    "        \"Inbound_Passengers\": \"Passengers_In\",\n",
    "        \"Inbound_Freight\": \"Freight_In\",\n",
    "        \"Inbound_Mail\": \"Mail_In\",\n",
    "        \"Outbound_Passengers\": \"Passengers_Out\",\n",
    "        \"Outbound_Freight\": \"Freight_Out\",\n",
    "        \"Outbound_Mail\": \"Mail_Out\",\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in ren.items() if k in df.columns})\n",
    "\n",
    "    # 3) Coerce numerics\n",
    "    for c in [\"Passengers_In\",\"Freight_In\",\"Mail_In\",\"Passengers_Out\",\"Freight_Out\",\"Mail_Out\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = _to_number(df[c])\n",
    "\n",
    "    # 4) Inject Year/Month + clean airline / totals flag\n",
    "    df[\"Year\"] = year\n",
    "    df[\"Month\"] = month\n",
    "    df[\"Is_Total_AllServices\"] = (\n",
    "        df.get(\"Airline_Name\", pd.Series(\"\", index=df.index)).astype(str).str.contains(\"ALL SERVICES\", case=False, na=False) |\n",
    "        df.get(\"Country\", pd.Series(\"\", index=df.index)).astype(str).str.contains(\"ALL SERVICES\", case=False, na=False)\n",
    "    )\n",
    "    # Strip \"ALL SERVICES\" text from Airline_Name\n",
    "    if \"Airline_Name\" in df.columns:\n",
    "        df[\"Airline_Name\"] = df[\"Airline_Name\"].astype(str).str.replace(r\"\\s*ALL SERVICES\\s*\", \"\", regex=True).str.strip()\n",
    "    # If Country says ALL SERVICES, null it (to avoid double-counting by country)\n",
    "    if \"Country\" in df.columns:\n",
    "        df.loc[df[\"Country\"].astype(str).str.contains(\"ALL SERVICES\", case=False, na=False), \"Country\"] = np.nan\n",
    "\n",
    "    # 5) Build long/tidy (Inbound + Outbound)\n",
    "    keys = [\"Airline_Name\",\"Country\",\"Year\",\"Month\"]\n",
    "    keys = [k for k in keys if k in df.columns]\n",
    "\n",
    "    inbound = df[keys].copy()\n",
    "    inbound[\"Direction\"] = \"Inbound\"\n",
    "    inbound[\"Passengers\"] = df.get(\"Passengers_In\", np.nan)\n",
    "    inbound[\"Freight_tonnes\"] = df.get(\"Freight_In\", np.nan)\n",
    "    inbound[\"Mail_tonnes\"] = df.get(\"Mail_In\", np.nan)\n",
    "\n",
    "    outbound = df[keys].copy()\n",
    "    outbound[\"Direction\"] = \"Outbound\"\n",
    "    outbound[\"Passengers\"] = df.get(\"Passengers_Out\", np.nan)\n",
    "    outbound[\"Freight_tonnes\"] = df.get(\"Freight_Out\", np.nan)\n",
    "    outbound[\"Mail_tonnes\"] = df.get(\"Mail_Out\", np.nan)\n",
    "\n",
    "    out = pd.concat([inbound, outbound], ignore_index=True)\n",
    "\n",
    "    # 6) Bring across the totals flag\n",
    "    out[\"Is_Total_AllServices\"] = df[\"Is_Total_AllServices\"].repeat(2).reset_index(drop=True)\n",
    "\n",
    "    # 7) YearMonthDate and final order\n",
    "    out[\"YearMonthDate\"] = pd.to_datetime(dict(year=out[\"Year\"], month=out[\"Month\"], day=1), errors=\"coerce\")\n",
    "    cols = [\"Year\",\"Month\",\"YearMonthDate\",\"Airline_Name\",\"Country\",\"Direction\",\n",
    "            \"Passengers\",\"Freight_tonnes\",\"Mail_tonnes\",\"Is_Total_AllServices\"]\n",
    "    out = out[cols].sort_values([\"Airline_Name\",\"Country\",\"Direction\"], na_position=\"last\").reset_index(drop=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1742f2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Year  Month YearMonthDate                Airline_Name       Country Direction  Passengers  Freight_tonnes  Mail_tonnes  Is_Total_AllServices\n",
      " 2025      5    2025-05-01 Air Caledonie International New Caledonia   Inbound      2653.0             2.1          0.2                 False\n",
      " 2025      5    2025-05-01 Air Caledonie International New Caledonia  Outbound      2500.0            40.2          1.9                 False\n",
      " 2025      5    2025-05-01                  Air Canada        Canada   Inbound     12287.0           277.0          9.1                 False\n",
      " 2025      5    2025-05-01                  Air Canada        Canada  Outbound     13749.0           458.2         15.9                  True\n",
      " 2025      5    2025-05-01                   Air China         China   Inbound      7591.0           253.0          NaN                 False\n",
      " 2025      5    2025-05-01                   Air China         China  Outbound      8098.0           132.4          1.9                  True\n",
      " 2025      5    2025-05-01                   Air India         India   Inbound     14028.0           412.1         11.2                 False\n",
      " 2025      5    2025-05-01                   Air India         India  Outbound     13322.0           407.1          NaN                 False\n",
      "Rows: 270\n"
     ]
    }
   ],
   "source": [
    "# Load your raw file\n",
    "raw_path = \"../data/raw/International_airline_activity_0525_Table1.csv\"\n",
    "t1_raw = pd.read_csv(raw_path)\n",
    "\n",
    "# Clean it (set year/month if not May 2025)\n",
    "t1_may = clean_table1_may_from_sample(t1_raw, year=2025, month=5)\n",
    "\n",
    "# Save for Power BI\n",
    "clean_path = \"../data/clean/Table1_May2025_clean.csv\"\n",
    "t1_may.to_csv(clean_path, index=False)\n",
    "\n",
    "print(t1_may.head(8).to_string(index=False))\n",
    "print(\"Rows:\", len(t1_may))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc03b4a",
   "metadata": {},
   "source": [
    "# Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19427950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_percent_norm(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Accepts values like 45.2 (pp), 0.9, '12.3%', returns 0..1 floats.\"\"\"\n",
    "    x = s.astype(str).str.strip().str.replace(\",\", \".\", regex=False)\n",
    "    pct = x.str.endswith(\"%\")\n",
    "    x = x.str.rstrip(\"%\")\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    # If any value > 1.5 (likely percent points), treat whole column as pp and divide by 100\n",
    "    if (pd.Series(x).dropna() > 1.5).any() or pct.any():\n",
    "        x = x / 100.0\n",
    "    return x\n",
    "\n",
    "def clean_table2_may_from_sample(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 1) normalise headers\n",
    "    df = df_raw.rename(columns=lambda c: str(c).strip().replace(\" \", \"_\")).copy()\n",
    "\n",
    "    # 2) rename key cols\n",
    "    ren = {\n",
    "        \"Scheduled_Operators\": \"Airline_Name\",\n",
    "        \"Country_to/from\": \"Country\",\n",
    "        \"Total_Passengers_2024\": \"Passengers_May_LY\",\n",
    "        \"Total_Passengers_2025\": \"Passengers_May\",\n",
    "        \"Total_Passengers_Perc_TOTAL\": \"MarketShare_Passengers\",\n",
    "        \"Total_Passengers_Perc_Change\": \"Passengers_YoY\",\n",
    "\n",
    "        \"Total_Freight_(tonnes)_2024\": \"Freight_2024_t\",\n",
    "        \"Total_Freight_(tonnes)_2025\": \"Freight_2025_t\",\n",
    "        \"Total_Freight_(tonnes)_Perc_TOTAL\": \"MarketShare_Freight\",\n",
    "        \"Total_Freight_(tonnes)_Perc_Change\": \"Freight_YoY\",\n",
    "\n",
    "        \"Total_Mail_(tonnes)_2024\": \"Mail_2024_t\",\n",
    "        \"Total_Mail_(tonnes)_2025\": \"Mail_2025_t\",\n",
    "        \"Total_Mail_(tonnes)_Perc_TOTAL\": \"MarketShare_Mail\",\n",
    "        \"Total_Mail_(tonnes)_Perc_Change\": \"Mail_YoY\",\n",
    "    }\n",
    "    df.rename(columns={k: v for k, v in ren.items() if k in df.columns}, inplace=True)\n",
    "\n",
    "    # 3) clean airline names (remove footnote suffixes)\n",
    "    if \"Airline_Name\" in df:\n",
    "        df[\"Airline_Name\"] = (\n",
    "            df[\"Airline_Name\"]\n",
    "            .astype(str)\n",
    "            .str.replace(r\"\\s*\\([a-z]\\)\\s*$\", \"\", regex=True)  # e.g., \" (a)\"\n",
    "            .str.strip()\n",
    "        )\n",
    "\n",
    "    # 4) detect & flag totals rows\n",
    "    df[\"Is_Total_AllServices\"] = (\n",
    "        df.get(\"Country\", pd.Series(\"\", index=df.index)).astype(str)\n",
    "          .str.contains(\"ALL SERVICES\", case=False, na=False)\n",
    "        |\n",
    "        df.get(\"Airline_Name\", pd.Series(\"\", index=df.index)).astype(str)\n",
    "          .str.contains(\"ALL SERVICES\", case=False, na=False)\n",
    "    )\n",
    "\n",
    "    # 5) numbers & percents\n",
    "    for c in [\"Passengers_May_LY\",\"Passengers_May\",\n",
    "              \"Freight_2024_t\",\"Freight_2025_t\",\n",
    "              \"Mail_2024_t\",\"Mail_2025_t\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = _to_number(df[c])\n",
    "\n",
    "    for c in [\"MarketShare_Passengers\",\"Passengers_YoY\",\n",
    "              \"MarketShare_Freight\",\"Freight_YoY\",\n",
    "              \"MarketShare_Mail\",\"Mail_YoY\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = _to_percent_norm(df[c])\n",
    "\n",
    "    # 6) optional: drop totals rows from the analysis table\n",
    "    df_clean = df[~df[\"Is_Total_AllServices\"]].copy()\n",
    "\n",
    "    # 7) select tidy passenger-focused output (keep freight/mail if you want)\n",
    "    keep_cols = [c for c in [\n",
    "        \"Airline_Name\",\"Country\",\n",
    "        \"Passengers_May\",\"Passengers_May_LY\",\"Passengers_YoY\",\"MarketShare_Passengers\",\n",
    "        \"Freight_2024_t\",\"Freight_2025_t\",\"Freight_YoY\",\"MarketShare_Freight\",\n",
    "        \"Mail_2024_t\",\"Mail_2025_t\",\"Mail_YoY\",\"MarketShare_Mail\"\n",
    "    ] if c in df_clean.columns]\n",
    "\n",
    "    out = df_clean[keep_cols].sort_values([\"Airline_Name\",\"Country\"]).reset_index(drop=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d79d9a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Airline_Name          Country  Passengers_May  Passengers_May_LY  Passengers_YoY  MarketShare_Passengers  Freight_2024_t  Freight_2025_t  Freight_YoY  MarketShare_Freight  Mail_2024_t  Mail_2025_t  Mail_YoY  MarketShare_Mail\n",
      "Air Caledonie International    New Caledonia          5153.0             3548.0           0.452                   0.002            22.7            42.3        0.863                0.000          0.4          2.1     3.964             0.001\n",
      "                 Air Canada           Canada         26036.0            27444.0          -0.051                   0.008           906.2           735.2       -0.189                0.008         33.2         25.0    -0.247             0.012\n",
      "                  Air China            China         15689.0            17975.0          -0.127                   0.005           722.4           385.3       -0.467                0.004          2.8          1.9    -0.329             0.001\n",
      "                  Air India            India         27350.0            31863.0          -0.142                   0.008           627.6           819.1        0.305                0.009         26.1         11.2    -0.571             0.005\n",
      "              Air Mauritius        Mauritius          3671.0             4054.0          -0.094                   0.001            91.1            56.3       -0.381                0.001          NaN          NaN       NaN               NaN\n",
      "            Air New Zealand      New Zealand        215882.0           213912.0           0.009                   0.064          3589.9          2973.1       -0.172                0.033        159.6        220.8     0.384             0.102\n",
      "                Air Niugini Papua New Guinea         11141.0            10350.0           0.076                   0.003           417.9           371.9       -0.110                0.004          NaN          NaN       NaN               NaN\n",
      "                Air Vanuatu          Vanuatu             NaN             1589.0          -1.000                     NaN             2.8             NaN       -1.000                  NaN          0.9          NaN    -1.000               NaN\n",
      "Rows: 128\n"
     ]
    }
   ],
   "source": [
    "raw_path = \"../data/raw/International_airline_activity_0525_Table2.csv\"\n",
    "t2_raw = pd.read_csv(raw_path)\n",
    "\n",
    "t2_may = clean_table2_may_from_sample(t2_raw)\n",
    "t2_may.to_csv(\"../data/clean/Table2_May2025_clean.csv\", index=False)\n",
    "\n",
    "print(t2_may.head(8).to_string(index=False))\n",
    "print(\"Rows:\", len(t2_may))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f68d7",
   "metadata": {},
   "source": [
    "# table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35d5f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_percent_norm(s: pd.Series) -> pd.Series:\n",
    "    x = s.astype(str).str.strip().str.replace(\",\", \".\", regex=False)\n",
    "    x = x.str.rstrip(\"%\")\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    # If values look like 0–100, convert to 0–1\n",
    "    return np.where(x > 1.5, x/100.0, x)\n",
    "\n",
    "def clean_table3_may_from_sample(df_raw: pd.DataFrame, year=2025, month=5) -> pd.DataFrame:\n",
    "    # 1) Standardise headers\n",
    "    df = df_raw.rename(columns=lambda c: str(c).strip().replace(\" \", \"_\")).copy()\n",
    "\n",
    "    # 2) Rename to canonical names\n",
    "    ren = {\n",
    "        \"Scheduled_Operator\": \"Airline_Name\",\n",
    "        \"Service_to/from\": \"Service_Region\",\n",
    "\n",
    "        \"Inbound_No._of_Flights\": \"In_Flights\",\n",
    "        \"Inbound_Pax_Carried\": \"In_Passengers\",\n",
    "        \"Inbound_Seats_Available\": \"In_Seats\",\n",
    "        \"Inbound_Seat_Utilisation_%\": \"In_LoadFactor\",\n",
    "\n",
    "        \"Outbound_No._of_Flights\": \"Out_Flights\",\n",
    "        \"Outbound_Pax_Carried\": \"Out_Passengers\",\n",
    "        \"Outbound_Seats_Available\": \"Out_Seats\",\n",
    "        \"Outbound_Seat_Utilisation_%\": \"Out_LoadFactor\",\n",
    "    }\n",
    "    df.rename(columns={k: v for k, v in ren.items() if k in df.columns}, inplace=True)\n",
    "\n",
    "    # 3) Clean airline (remove trailing footnote markers like \" (a)\")\n",
    "    if \"Airline_Name\" in df:\n",
    "        df[\"Airline_Name\"] = (\n",
    "            df[\"Airline_Name\"]\n",
    "            .astype(str)\n",
    "            .str.replace(r\"\\s*\\([a-z]\\)\\s*$\", \"\", regex=True)\n",
    "            .str.replace(r\"\\s*ALL SERVICES\\s*\", \"\", regex=True)\n",
    "            .str.strip()\n",
    "        )\n",
    "\n",
    "    # 4) Flag totals rows\n",
    "    df[\"Is_Total_AllServices\"] = (\n",
    "        df.get(\"Service_Region\", pd.Series(\"\", index=df.index)).astype(str)\n",
    "          .str.contains(\"ALL SERVICES\", case=False, na=False)\n",
    "        |\n",
    "        df.get(\"Airline_Name\", pd.Series(\"\", index=df.index)).astype(str)\n",
    "          .str.contains(\"ALL SERVICES\", case=False, na=False)\n",
    "    )\n",
    "\n",
    "    # 5) Numeric coercion\n",
    "    for c in [\"In_Flights\",\"In_Passengers\",\"In_Seats\",\"Out_Flights\",\"Out_Passengers\",\"Out_Seats\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = _to_number(df[c])\n",
    "\n",
    "    for c in [\"In_LoadFactor\",\"Out_LoadFactor\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = _to_percent_norm(df[c])\n",
    "\n",
    "    # 6) Add Year/Month\n",
    "    df[\"Year\"] = year\n",
    "    df[\"Month\"] = month\n",
    "\n",
    "    # 7) Build tidy long format: Inbound + Outbound\n",
    "    keys = [k for k in [\"Airline_Name\",\"Service_Region\",\"Year\",\"Month\"] if k in df.columns]\n",
    "\n",
    "    inbound = df[keys].copy()\n",
    "    inbound[\"Direction\"] = \"Inbound\"\n",
    "    inbound[\"Flights\"] = df.get(\"In_Flights\", np.nan)\n",
    "    inbound[\"Passengers\"] = df.get(\"In_Passengers\", np.nan)\n",
    "    inbound[\"Seats_Available\"] = df.get(\"In_Seats\", np.nan)\n",
    "    inbound[\"Seat_Utilisation\"] = df.get(\"In_LoadFactor\", np.nan)\n",
    "\n",
    "    outbound = df[keys].copy()\n",
    "    outbound[\"Direction\"] = \"Outbound\"\n",
    "    outbound[\"Flights\"] = df.get(\"Out_Flights\", np.nan)\n",
    "    outbound[\"Passengers\"] = df.get(\"Out_Passengers\", np.nan)\n",
    "    outbound[\"Seats_Available\"] = df.get(\"Out_Seats\", np.nan)\n",
    "    outbound[\"Seat_Utilisation\"] = df.get(\"Out_LoadFactor\", np.nan)\n",
    "\n",
    "    out = pd.concat([inbound, outbound], ignore_index=True)\n",
    "\n",
    "    # 8) Carry totals flag & YearMonthDate\n",
    "    out[\"Is_Total_AllServices\"] = df[\"Is_Total_AllServices\"].repeat(2).reset_index(drop=True)\n",
    "    out[\"YearMonthDate\"] = pd.to_datetime(dict(year=out[\"Year\"], month=out[\"Month\"], day=1), errors=\"coerce\")\n",
    "\n",
    "    # 9) Final ordering (drop totals rows for analysis if you want)\n",
    "    out = out[[\n",
    "        \"Year\",\"Month\",\"YearMonthDate\",\n",
    "        \"Airline_Name\",\"Service_Region\",\"Direction\",\n",
    "        \"Flights\",\"Passengers\",\"Seats_Available\",\"Seat_Utilisation\",\n",
    "        \"Is_Total_AllServices\"\n",
    "    ]]\n",
    "\n",
    "    # If you want to exclude ALL SERVICES totals from visuals:\n",
    "    out = out[~out[\"Is_Total_AllServices\"]].reset_index(drop=True)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2833df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Year  Month YearMonthDate                Airline_Name   Service_Region Direction  Flights  Passengers  Seats_Available  Seat_Utilisation  Is_Total_AllServices\n",
      " 2025      5    2025-05-01 Air Caledonie International    New Caledonia   Inbound     17.0      2653.0           3643.0             0.728                 False\n",
      " 2025      5    2025-05-01                  Air Canada           Canada   Inbound     53.0     12287.0          15941.0             0.771                 False\n",
      " 2025      5    2025-05-01                   Air China            China   Inbound     31.0      7591.0           9031.0             0.841                 False\n",
      " 2025      5    2025-05-01                   Air India            India   Inbound     62.0     14028.0          15872.0             0.884                 False\n",
      " 2025      5    2025-05-01               Air Mauritius        Mauritius   Inbound      8.0      1788.0           2350.0             0.761                 False\n",
      " 2025      5    2025-05-01             Air New Zealand      New Zealand   Inbound    601.0    111435.0         126898.0             0.878                 False\n",
      " 2025      5    2025-05-01                 Air Niugini Papua New Guinea   Inbound     85.0      5872.0          10762.0             0.546                 False\n",
      " 2025      5    2025-05-01              AirAsia Berhad         Malaysia   Inbound     33.0      7008.0           7788.0             0.900                 False\n",
      "Rows: 222\n"
     ]
    }
   ],
   "source": [
    "t3_raw = pd.read_csv(\"../data/raw/International_airline_activity_0525_Table3.csv\")\n",
    "t3_may = clean_table3_may_from_sample(t3_raw, year=2025, month=5)\n",
    "\n",
    "t3_may.to_csv(\"../data/clean/Table3_May2025_clean.csv\", index=False)\n",
    "print(t3_may.head(8).to_string(index=False))\n",
    "print(\"Rows:\", len(t3_may))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec6b8d6",
   "metadata": {},
   "source": [
    "# TABLE 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce43fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "RAW   = Path(\"../data/raw\")\n",
    "CLEAN = Path(\"../data/clean\")\n",
    "CLEAN.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def to_number(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"'1,234' -> 1234.0 ; '..'/'-' -> NaN\"\"\"\n",
    "    return (\n",
    "        s.astype(str)\n",
    "         .str.strip()\n",
    "         .replace({\"..\": np.nan, \"—\": np.nan, \"-\": np.nan, \"\": np.nan})\n",
    "         .str.replace(\",\", \"\", regex=False)\n",
    "         .astype(float)\n",
    "    )\n",
    "\n",
    "def to_percent_0_1(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"26.1 or '26.1%' -> 0.261 ; 0.26 stays 0.26\"\"\"\n",
    "    x = s.astype(str).str.strip()\n",
    "    has_pct = x.str.endswith(\"%\")\n",
    "    x = x.str.rstrip(\"%\")\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    # If any value looks like percent-points (>1.5), divide by 100\n",
    "    if pd.notna(x).any() and (x.dropna() > 1.5).any() or has_pct.any():\n",
    "        x = x / 100.0\n",
    "    return x\n",
    "\n",
    "# ---------- cleaner ----------\n",
    "def clean_table4_from_sample(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "    # standardise headers a bit (keep original labels)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # replace '..' with NaN across the board\n",
    "    df = df.replace(\"..\", np.nan)\n",
    "\n",
    "    # numeric columns (everything except City, Indicator)\n",
    "    num_cols = [c for c in df.columns if c not in [\"City\", \"Indicator\"]]\n",
    "\n",
    "    # convert numerics\n",
    "    for c in num_cols:\n",
    "        df[c] = to_number(df[c])\n",
    "\n",
    "    # keep % change both as percent-points and 0-1\n",
    "    for c in [\"Inbound_Perc_Change\", \"Outbound_Perc_Change\"]:\n",
    "        if c in df.columns:\n",
    "            df[c + \"_0_1\"] = to_percent_0_1(df[c])\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------- reshape: long ----------\n",
    "def table4_to_long(df_clean: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Output columns:\n",
    "    City | Metric | Direction | Year | Value\n",
    "    Metric ∈ {'Passengers','Freight (Tonnes)','Aircraft Movements'}\n",
    "    Direction ∈ {'Inbound','Outbound'}\n",
    "    Year ∈ {2024, 2025}\n",
    "    \"\"\"\n",
    "    # melt inbound/outbound 2024/2025\n",
    "    long_vals = df_clean.melt(\n",
    "        id_vars=[\"City\", \"Indicator\", \"Inbound_Perc_Change\", \"Inbound_Perc_Change_0_1\",\n",
    "                 \"Outbound_Perc_Change\", \"Outbound_Perc_Change_0_1\"],\n",
    "        value_vars=[\"Inbound_2024\",\"Inbound_2025\",\"Outbound_2024\",\"Outbound_2025\"],\n",
    "        var_name=\"DirectionYear\", value_name=\"Value\"\n",
    "    )\n",
    "\n",
    "    # split Direction & Year\n",
    "    long_vals[\"Direction\"] = np.where(long_vals[\"DirectionYear\"].str.startswith(\"Inbound\"), \"Inbound\", \"Outbound\")\n",
    "    long_vals[\"Year\"] = long_vals[\"DirectionYear\"].str.extract(r\"(2024|2025)\").astype(int)\n",
    "    long_vals = long_vals.drop(columns=[\"DirectionYear\"])\n",
    "\n",
    "    # rename for clarity\n",
    "    long_vals = long_vals.rename(columns={\"Indicator\":\"Metric\"})\n",
    "\n",
    "    # Optional: bring the corresponding YoY for that direction\n",
    "    long_vals[\"Perc_Change_pp\"] = np.where(\n",
    "        long_vals[\"Direction\"]==\"Inbound\",\n",
    "        long_vals[\"Inbound_Perc_Change\"],\n",
    "        long_vals[\"Outbound_Perc_Change\"]\n",
    "    )\n",
    "    long_vals[\"Perc_Change_0_1\"] = np.where(\n",
    "        long_vals[\"Direction\"]==\"Inbound\",\n",
    "        long_vals[\"Inbound_Perc_Change_0_1\"],\n",
    "        long_vals[\"Outbound_Perc_Change_0_1\"]\n",
    "    )\n",
    "\n",
    "    # final ordering\n",
    "    cols = [\"City\",\"Metric\",\"Direction\",\"Year\",\"Value\",\"Perc_Change_pp\",\"Perc_Change_0_1\"]\n",
    "    return long_vals[cols].sort_values([\"City\",\"Metric\",\"Direction\",\"Year\"]).reset_index(drop=True)\n",
    "\n",
    "# ---------- reshape: wide per city ----------\n",
    "def table4_to_wide(df_clean: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One row per city. Columns like:\n",
    "      Passengers_In_2024, Passengers_Out_2024, Freight (Tonnes)_In_2025, ...\n",
    "      + Perc change columns for inbound/outbound per metric\n",
    "    \"\"\"\n",
    "    # pivot values\n",
    "    def pivot_metric(metric_name):\n",
    "        sub = df_clean[df_clean[\"Indicator\"] == metric_name].copy()\n",
    "        keep = [\"City\",\n",
    "                \"Inbound_2024\",\"Inbound_2025\",\"Inbound_Perc_Change\",\"Inbound_Perc_Change_0_1\",\n",
    "                \"Outbound_2024\",\"Outbound_2025\",\"Outbound_Perc_Change\",\"Outbound_Perc_Change_0_1\"]\n",
    "        sub = sub[keep]\n",
    "        sub = sub.rename(columns={\n",
    "            \"Inbound_2024\": f\"{metric_name}_In_2024\",\n",
    "            \"Inbound_2025\": f\"{metric_name}_In_2025\",\n",
    "            \"Inbound_Perc_Change\": f\"{metric_name}_In_YoY_pp\",\n",
    "            \"Inbound_Perc_Change_0_1\": f\"{metric_name}_In_YoY\",\n",
    "            \"Outbound_2024\": f\"{metric_name}_Out_2024\",\n",
    "            \"Outbound_2025\": f\"{metric_name}_Out_2025\",\n",
    "            \"Outbound_Perc_Change\": f\"{metric_name}_Out_YoY_pp\",\n",
    "            \"Outbound_Perc_Change_0_1\": f\"{metric_name}_Out_YoY\",\n",
    "        })\n",
    "        return sub\n",
    "\n",
    "    m1 = pivot_metric(\"Passengers\")\n",
    "    m2 = pivot_metric(\"Freight (Tonnes)\")\n",
    "    m3 = pivot_metric(\"Aircraft Movements\")\n",
    "\n",
    "    # merge on City\n",
    "    wide = m1.merge(m2, on=\"City\", how=\"outer\").merge(m3, on=\"City\", how=\"outer\")\n",
    "    return wide.sort_values(\"City\").reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce9895cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote: ../data/clean/Table4_Cities_clean.csv ../data/clean/Table4_Cities_long.csv ../data/clean/Table4_Cities_wide.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- run ----------\n",
    "# Replace with your path/file name\n",
    "t4_raw_path = RAW / \"International_airline_activity_0525_Table4.csv\"\n",
    "t4_raw = pd.read_csv(t4_raw_path)\n",
    "\n",
    "t4_clean = clean_table4_from_sample(t4_raw)\n",
    "t4_long  = table4_to_long(t4_clean)\n",
    "t4_wide  = table4_to_wide(t4_clean)\n",
    "\n",
    "# save\n",
    "t4_clean.to_csv(CLEAN / \"Table4_Cities_clean.csv\", index=False)\n",
    "t4_long.to_csv(CLEAN / \"Table4_Cities_long.csv\", index=False)\n",
    "t4_wide.to_csv(CLEAN / \"Table4_Cities_wide.csv\", index=False)\n",
    "\n",
    "print(\"✅ Wrote:\",\n",
    "      CLEAN / \"Table4_Cities_clean.csv\",\n",
    "      CLEAN / \"Table4_Cities_long.csv\",\n",
    "      CLEAN / \"Table4_Cities_wide.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e2945",
   "metadata": {},
   "source": [
    "# TABLE 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8edb3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- cleaner ----------\n",
    "def clean_table5_from_sample(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "    # Standardise headers a little\n",
    "    df.columns = [c.strip().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "    # Canonical rename\n",
    "    ren = {\n",
    "        \"Foreign_Port\": \"Foreign_City\",\n",
    "        \"Australian_Port\": \"Australian_City\",\n",
    "        \"Passengers_2024_Inbound\":  \"Pax_2024_In\",\n",
    "        \"Passengers_2024_Outbound\": \"Pax_2024_Out\",\n",
    "        \"Passengers_2025_Inbound\":  \"Pax_2025_In\",\n",
    "        \"Passengers_2025_Outbound\": \"Pax_2025_Out\",\n",
    "        \"Freight_(tonnes)_2024_Inbound\":  \"Freight_2024_In_t\",\n",
    "        \"Freight_(tonnes)_2024_Outbound\": \"Freight_2024_Out_t\",\n",
    "        \"Freight_(tonnes)_2025_Inbound\":  \"Freight_2025_In_t\",\n",
    "        \"Freight_(tonnes)_2025_Outbound\": \"Freight_2025_Out_t\",\n",
    "    }\n",
    "    df.rename(columns={k:v for k,v in ren.items() if k in df.columns}, inplace=True)\n",
    "\n",
    "    # Replace textual missing with NaN\n",
    "    df = df.replace({\"..\": np.nan})\n",
    "\n",
    "    # Numeric coercion\n",
    "    num_cols = [c for c in df.columns if c not in [\"Foreign_City\",\"Australian_City\"]]\n",
    "    for c in num_cols:\n",
    "        df[c] = to_number(df[c])\n",
    "\n",
    "    # Flag aggregates (optional: you can filter them out later)\n",
    "    df[\"Is_Total_Row\"] = df[\"Foreign_City\"].astype(str).str.fullmatch(r\"(?i)total\") | \\\n",
    "                         df[\"Australian_City\"].astype(str).str.fullmatch(r\"(?i)australia\")\n",
    "\n",
    "    # Consistent city text\n",
    "    for c in [\"Foreign_City\",\"Australian_City\"]:\n",
    "        if c in df:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------- to long (tidy) ----------\n",
    "def table5_to_long(df_clean: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Output columns:\n",
    "      Australian_City | Foreign_City | Metric | Direction | Year | Value\n",
    "    Metric ∈ {'Passengers','Freight_tonnes'}\n",
    "    Direction ∈ {'Inbound','Outbound'}\n",
    "    Year ∈ {2024, 2025}\n",
    "    \"\"\"\n",
    "    # Build tidy for passengers\n",
    "    pax_long = df_clean.melt(\n",
    "        id_vars=[\"Australian_City\",\"Foreign_City\",\"Is_Total_Row\"],\n",
    "        value_vars=[\"Pax_2024_In\",\"Pax_2024_Out\",\"Pax_2025_In\",\"Pax_2025_Out\"],\n",
    "        var_name=\"key\", value_name=\"Value\"\n",
    "    )\n",
    "    pax_long[\"Metric\"] = \"Passengers\"\n",
    "    pax_long[\"Direction\"] = np.where(pax_long[\"key\"].str.contains(\"_In$\"), \"Inbound\", \"Outbound\")\n",
    "    pax_long[\"Year\"] = pax_long[\"key\"].str.extract(r\"(\\d{4})\").astype(int)\n",
    "    pax_long = pax_long.drop(columns=[\"key\"])\n",
    "\n",
    "    # Build tidy for freight (if present)\n",
    "    freight_cols = [c for c in df_clean.columns if c.startswith(\"Freight_\")]\n",
    "    if freight_cols:\n",
    "        fr_long = df_clean.melt(\n",
    "            id_vars=[\"Australian_City\",\"Foreign_City\",\"Is_Total_Row\"],\n",
    "            value_vars=freight_cols,\n",
    "            var_name=\"key\", value_name=\"Value\"\n",
    "        )\n",
    "        fr_long[\"Metric\"] = \"Freight_tonnes\"\n",
    "        fr_long[\"Direction\"] = np.where(fr_long[\"key\"].str.contains(\"_In_t$\"), \"Inbound\", \"Outbound\")\n",
    "        fr_long[\"Year\"] = fr_long[\"key\"].str.extract(r\"(\\d{4})\").astype(int)\n",
    "        fr_long = fr_long.drop(columns=[\"key\"])\n",
    "        long = pd.concat([pax_long, fr_long], ignore_index=True)\n",
    "    else:\n",
    "        long = pax_long\n",
    "\n",
    "    # Sort & return\n",
    "    long = long[[\"Australian_City\",\"Foreign_City\",\"Metric\",\"Direction\",\"Year\",\"Value\",\"Is_Total_Row\"]]\\\n",
    "             .sort_values([\"Australian_City\",\"Foreign_City\",\"Metric\",\"Direction\",\"Year\"])\\\n",
    "             .reset_index(drop=True)\n",
    "    return long\n",
    "\n",
    "# ---------- quick “top routes 2025” helper ----------\n",
    "def top_routes_2025_passengers(long_df: pd.DataFrame, top_n=100, exclude_totals=True) -> pd.DataFrame:\n",
    "    sub = long_df[(long_df[\"Metric\"]==\"Passengers\") & (long_df[\"Year\"]==2025)].copy()\n",
    "    if exclude_totals:\n",
    "        sub = sub[~sub[\"Is_Total_Row\"]]\n",
    "    # Sum inbound + outbound to get total flow per city pair\n",
    "    agg = (sub.groupby([\"Australian_City\",\"Foreign_City\"], as_index=False)\n",
    "             .agg(Passengers_2025_Total=(\"Value\",\"sum\")))\n",
    "    return agg.sort_values(\"Passengers_2025_Total\", ascending=False).head(top_n).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cc800db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote: ../data/clean/Table5_pairs_clean.csv ../data/clean/Table5_pairs_long.csv ../data/clean/Table5_pairs_toproutes_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- run ----------\n",
    "t5_raw_path = RAW / \"International_airline_activity_0525_Table5.csv\"  # adjust if needed\n",
    "t5_raw  = pd.read_csv(t5_raw_path)\n",
    "\n",
    "t5_clean = clean_table5_from_sample(t5_raw)\n",
    "t5_long  = table5_to_long(t5_clean)\n",
    "t5_top   = top_routes_2025_passengers(t5_long, top_n=100, exclude_totals=True)\n",
    "\n",
    "t5_clean.to_csv(CLEAN / \"Table5_pairs_clean.csv\", index=False)\n",
    "t5_long.to_csv(CLEAN / \"Table5_pairs_long.csv\", index=False)\n",
    "t5_top.to_csv(CLEAN / \"Table5_pairs_toproutes_2025.csv\", index=False)\n",
    "\n",
    "print(\"✅ Wrote:\",\n",
    "      CLEAN / \"Table5_pairs_clean.csv\",\n",
    "      CLEAN / \"Table5_pairs_long.csv\",\n",
    "      CLEAN / \"Table5_pairs_toproutes_2025.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0fdc0",
   "metadata": {},
   "source": [
    "1) Build dimensions (once, then reuse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a355cf3",
   "metadata": {},
   "source": [
    "dim_date (month grain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fda50b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def build_dim_date(start=\"2009-01-01\", end=\"2025-12-01\"):\n",
    "    dates = pd.date_range(start=start, end=end, freq=\"MS\")  # Month start\n",
    "    df = pd.DataFrame({\"YearMonthDate\": dates})\n",
    "    df[\"DateID\"]     = df[\"YearMonthDate\"].dt.strftime(\"%Y%m\").astype(int)\n",
    "    df[\"Year\"]       = df[\"YearMonthDate\"].dt.year\n",
    "    df[\"Month\"]      = df[\"YearMonthDate\"].dt.month\n",
    "    df[\"MonthName\"]  = df[\"YearMonthDate\"].dt.strftime(\"%b\")\n",
    "    df[\"Quarter\"]    = \"Q\" + ((df[\"Month\"]-1)//3 + 1).astype(str)\n",
    "    return df\n",
    "\n",
    "dim_date = build_dim_date(\n",
    "    start=f\"{t1_hist['Year'].min()}-{t1_hist['Month'].min():02d}-01\",\n",
    "    end=f\"{t1_hist['Year'].max()}-{t1_hist['Month'].max():02d}-01\"\n",
    ")\n",
    "dim_date.to_csv(\"../data/clean/dim_date.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1d384d",
   "metadata": {},
   "source": [
    "dim_airline (from Table1 history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f94e79da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ dim_airline rows: 97  → ../data/clean/dim_airline.csv\n",
      "Duplicate Airline_Name rows: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "CLEAN = Path(\"../data/clean\")\n",
    "CLEAN.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 1) helpers ---\n",
    "def norm_airline(name: str) -> str:\n",
    "    if pd.isna(name): return None\n",
    "    s = str(name).strip()\n",
    "    s = re.sub(r\"\\s*\\([^)]*\\)\\s*$\", \"\", s)  # drop trailing footnote markers like \"(a)\"\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "# Known home-country overrides for major carriers (extend anytime)\n",
    "HOME_COUNTRY_OVERRIDE = {\n",
    "    \"Qantas Airways\": \"Australia\",\n",
    "    \"Jetstar\": \"Australia\",\n",
    "    \"Virgin Australia\": \"Australia\",\n",
    "    \"Air New Zealand\": \"New Zealand\",\n",
    "    \"Singapore Airlines\": \"Singapore\",\n",
    "    \"Scoot\": \"Singapore\",\n",
    "    \"Silk Air\": \"Singapore\",\n",
    "    \"Cathay Pacific Airways\": \"Hong Kong (SAR)\",\n",
    "    \"Hong Kong Airlines\": \"Hong Kong (SAR)\",\n",
    "    \"Emirates\": \"United Arab Emirates\",\n",
    "    \"Etihad Airways\": \"United Arab Emirates\",\n",
    "    \"Qatar Airways\": \"Qatar\",\n",
    "    \"Turkish Airlines\": \"Turkey\",\n",
    "    \"British Airways\": \"UK\",\n",
    "    \"American Airlines\": \"USA\",\n",
    "    \"Delta Air Lines\": \"USA\",\n",
    "    \"United Airlines\": \"USA\",\n",
    "    \"Air Canada\": \"Canada\",\n",
    "    \"Air China\": \"China\",\n",
    "    \"China Southern Airlines\": \"China\",\n",
    "    \"China Eastern Airlines\": \"China\",\n",
    "    \"Xiamen Airlines\": \"China\",\n",
    "    \"Hainan Airlines\": \"China\",\n",
    "    \"Tianjin Airlines\": \"China\",\n",
    "    \"Juneyao Air\": \"China\",\n",
    "    \"Sichuan Airlines\": \"China\",\n",
    "    \"Japan Airlines\": \"Japan\",\n",
    "    \"All Nippon Airways\": \"Japan\",\n",
    "    \"Korean Air\": \"Korea\",\n",
    "    \"Asiana Airlines\": \"Korea\",\n",
    "    \"Malaysia Airlines\": \"Malaysia\",\n",
    "    \"AirAsia Berhad\": \"Malaysia\",\n",
    "    \"AirAsia X\": \"Malaysia\",\n",
    "    \"Garuda Indonesia\": \"Indonesia\",\n",
    "    \"Batik Air Indonesia\": \"Indonesia\",\n",
    "    \"Indonesia AirAsia\": \"Indonesia\",\n",
    "    \"Philippine Airlines\": \"Philippines\",\n",
    "    \"Cebu Pacific Air\": \"Philippines\",\n",
    "    \"Vietnam Airlines\": \"Vietnam\",\n",
    "    \"Vietjet Air\": \"Vietnam\",\n",
    "    \"SriLankan Airlines\": \"Sri Lanka\",\n",
    "    \"Fiji Airways\": \"Fiji\",\n",
    "    \"Air Vanuatu\": \"Vanuatu\",\n",
    "    \"Nauru Airlines\": \"Nauru\",\n",
    "    \"Solomon Airlines\": \"Solomon Islands\",\n",
    "    \"LATAM Airlines\": \"Chile\",\n",
    "    \"LAN Airlines\": \"Chile\",\n",
    "    \"South African Airways\": \"South Africa\",\n",
    "    \"EVA Air\": \"Taiwan\",\n",
    "    \"China Airlines\": \"Taiwan\",\n",
    "    \"Royal Brunei Airlines\": \"Brunei\",\n",
    "    \"Tasman Cargo Airlines\": \"Australia\",\n",
    "}\n",
    "\n",
    "# --- 2) source: use your already-cleaned historical Table 1 ---\n",
    "# t1_hist must exist in memory. If not, load your cleaned CSV:\n",
    "# t1_hist = pd.read_csv(\"../data/clean/Table1_Historical_clean.csv\", parse_dates=[\"YearMonthDate\"])\n",
    "\n",
    "df = t1_hist.copy()\n",
    "\n",
    "# Normalise names and drop junk\n",
    "df[\"Airline_Name\"] = df[\"Airline_Name\"].apply(norm_airline)\n",
    "df = df[~df[\"Airline_Name\"].isin([None, \"\", \"ALL SERVICES\"])]\n",
    "\n",
    "# --- 3) infer home country from most frequent \"Country to/from\" seen historically ---\n",
    "# Note: this is a proxy; override map will correct the majors.\n",
    "mode_country = (\n",
    "    df.groupby([\"Airline_Name\",\"Country\"], dropna=False)\n",
    "      .size()\n",
    "      .reset_index(name=\"n\")\n",
    "      .sort_values([\"Airline_Name\",\"n\"], ascending=[True, False])\n",
    "      .drop_duplicates(subset=[\"Airline_Name\"])\n",
    "      .rename(columns={\"Country\":\"Inferred_Home_Country\"})\n",
    "      [[\"Airline_Name\",\"Inferred_Home_Country\"]]\n",
    ")\n",
    "\n",
    "# Unique airline list\n",
    "airlines = (\n",
    "    df[[\"Airline_Name\"]]\n",
    "      .drop_duplicates()\n",
    "      .sort_values(\"Airline_Name\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Apply overrides, else use inferred mode\n",
    "airlines = airlines.merge(mode_country, on=\"Airline_Name\", how=\"left\")\n",
    "airlines[\"Home_Country\"] = airlines[\"Airline_Name\"].map(HOME_COUNTRY_OVERRIDE)\\\n",
    "                             .fillna(airlines[\"Inferred_Home_Country\"])\n",
    "airlines.drop(columns=[\"Inferred_Home_Country\"], inplace=True)\n",
    "\n",
    "# Final tidy: if still missing, label \"Unknown\" (you can fix manually later)\n",
    "airlines[\"Home_Country\"] = airlines[\"Home_Country\"].fillna(\"Unknown\")\n",
    "\n",
    "# Assign surrogate key\n",
    "airlines.insert(0, \"AirlineID\", range(1, len(airlines)+1))\n",
    "\n",
    "dim_airline = airlines[[\"AirlineID\",\"Airline_Name\",\"Home_Country\"]]\n",
    "\n",
    "# Save\n",
    "dim_airline.to_csv(CLEAN / \"dim_airline.csv\", index=False)\n",
    "print(f\"✅ dim_airline rows: {len(dim_airline)}  → {CLEAN/'dim_airline.csv'}\")\n",
    "\n",
    "# Optional: quick sanity check for duplicates\n",
    "dups = dim_airline[\"Airline_Name\"].duplicated().sum()\n",
    "print(f\"Duplicate Airline_Name rows: {dups}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b05db8",
   "metadata": {},
   "source": [
    "2) Build fact tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8f20d",
   "metadata": {},
   "source": [
    "fact_airline_monthly (from Table 1 history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "883a0461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ fact_airline_monthly rows: 238\n",
      "Missing AirlineID: 0\n",
      "Missing DateID: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t1 = pd.read_csv(\"../data/clean/Table1_May2025_clean.csv\")   # Year, Month, Airline_Name, Country, Direction, Passengers, Freight_tonnes, Mail_tonnes, Is_Total_AllServices\n",
    "dim_airline = pd.read_csv(\"../data/clean/dim_airline.csv\")\n",
    "dim_date    = pd.read_csv(\"../data/clean/dim_date.csv\")      # has Year, Month, DateID\n",
    "\n",
    "# keep only non-total rows\n",
    "t1 = t1[t1[\"Is_Total_AllServices\"] == False].copy()\n",
    "\n",
    "# ensure ints for join\n",
    "for c in [\"Year\",\"Month\"]:\n",
    "    t1[c] = pd.to_numeric(t1[c], errors=\"coerce\").astype(\"Int64\")\n",
    "    dim_date[c] = pd.to_numeric(dim_date[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "f1 = (t1\n",
    "      .merge(dim_airline[[\"AirlineID\",\"Airline_Name\"]], on=\"Airline_Name\", how=\"left\")\n",
    "      .merge(dim_date[[\"DateID\",\"Year\",\"Month\"]], on=[\"Year\",\"Month\"], how=\"left\"))\n",
    "\n",
    "fact_airline_monthly = f1[[\n",
    "    \"DateID\",\"AirlineID\",\"Country\",\"Direction\",\n",
    "    \"Passengers\",\"Freight_tonnes\",\"Mail_tonnes\"\n",
    "]]\n",
    "\n",
    "fact_airline_monthly.to_csv(\"../data/clean/fact_airline_monthly.csv\", index=False)\n",
    "\n",
    "print(f\"✅ fact_airline_monthly rows: {len(fact_airline_monthly)}\")\n",
    "print(\"Missing AirlineID:\", fact_airline_monthly[\"AirlineID\"].isna().sum())\n",
    "print(\"Missing DateID:\", fact_airline_monthly[\"DateID\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a645bd9",
   "metadata": {},
   "source": [
    "fact_airline_flights (from Table 3 May clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5e50ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ fact_airline_flights rows: 222\n",
      "⚠️ AirlineID missing: 0\n",
      "⚠️ DateID missing: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Load ---\n",
    "t3 = pd.read_csv(\"../data/clean/Table3_May2025_clean.csv\")  # has Year, Month, Airline_Name, ...\n",
    "dim_airline = pd.read_csv(\"../data/clean/dim_airline.csv\")\n",
    "dim_date    = pd.read_csv(\"../data/clean/dim_date.csv\")     # must have Year, Month, DateID\n",
    "\n",
    "# --- Ensure Year/Month are numeric ---\n",
    "for c in [\"Year\", \"Month\"]:\n",
    "    if c in t3.columns:\n",
    "        t3[c] = pd.to_numeric(t3[c], errors=\"coerce\").astype(\"Int64\")\n",
    "    else:\n",
    "        # If Month isn't present (e.g., single-month extract), hardcode May=5 or set what applies\n",
    "        if c == \"Year\":\n",
    "            raise ValueError(\"t3 is missing 'Year' column.\")\n",
    "        if c == \"Month\":\n",
    "            t3[\"Month\"] = 5  # set to the correct month for this file\n",
    "\n",
    "# Same for dim_date\n",
    "for c in [\"Year\", \"Month\"]:\n",
    "    if c not in dim_date.columns:\n",
    "        raise ValueError(\"dim_date must include 'Year' and 'Month' columns.\")\n",
    "    dim_date[c] = pd.to_numeric(dim_date[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# --- Merge IDs (no YearMonthDate anywhere) ---\n",
    "f3 = (\n",
    "    t3\n",
    "    .merge(dim_airline[[\"AirlineID\",\"Airline_Name\"]], on=\"Airline_Name\", how=\"left\")\n",
    "    .merge(dim_date[[\"DateID\",\"Year\",\"Month\"]], on=[\"Year\",\"Month\"], how=\"left\")\n",
    ")\n",
    "\n",
    "# --- Build fact table ---\n",
    "fact_airline_flights = f3[[\n",
    "    \"DateID\",\"AirlineID\",\"Service_Region\",\"Direction\",\n",
    "    \"Flights\",\"Passengers\",\"Seats_Available\",\"Seat_Utilisation\"\n",
    "]]\n",
    "\n",
    "fact_airline_flights.to_csv(\"../data/clean/fact_airline_flights.csv\", index=False)\n",
    "\n",
    "# --- Diagnostics ---\n",
    "print(f\"✅ fact_airline_flights rows: {len(fact_airline_flights)}\")\n",
    "print(f\"⚠️ AirlineID missing: {fact_airline_flights['AirlineID'].isna().sum()}\")\n",
    "print(f\"⚠️ DateID missing: {fact_airline_flights['DateID'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb4ea47",
   "metadata": {},
   "source": [
    "fact_airport_totals (from Table 4 long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e977acf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ fact_airport_totals rows: 144\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Load\n",
    "t4_long = pd.read_csv(\"../data/clean/Table4_Cities_long.csv\")  # City, Metric, Direction, Year, Value\n",
    "dim_citycountry = pd.read_csv(\"../data/clean/dim_citycountry.csv\")\n",
    "dim_date = pd.read_csv(\"../data/clean/dim_date.csv\")\n",
    "\n",
    "# --- Normalise text\n",
    "t4_long[\"City\"] = t4_long[\"City\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "dim_citycountry[\"City\"] = dim_citycountry[\"City\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "# --- Map City → CityID\n",
    "f4 = t4_long.merge(\n",
    "    dim_citycountry.rename(columns={\"City\": \"CityName\"}),\n",
    "    left_on=\"City\",\n",
    "    right_on=\"CityName\",\n",
    "    how=\"left\"\n",
    ").drop(columns=[\"CityName\"])\n",
    "\n",
    "# --- Build YearMonthDate and join DateID\n",
    "f4[\"YearMonthDate\"] = pd.to_datetime(dict(year=f4[\"Year\"], month=1, day=1))\n",
    "dim_date[\"YearMonthDate\"] = pd.to_datetime(dim_date[\"YearMonthDate\"])\n",
    "f4 = f4.merge(dim_date[[\"DateID\", \"YearMonthDate\"]], on=\"YearMonthDate\", how=\"left\")\n",
    "\n",
    "# --- Final fact table\n",
    "fact_airport_totals = f4[[\"DateID\", \"CityID\", \"City\", \"Metric\", \"Direction\", \"Value\"]] \\\n",
    "    .rename(columns={\"Value\": \"Amount\"})\n",
    "\n",
    "# --- Optional: drop rows with missing CityID\n",
    "missing_city = fact_airport_totals[\"CityID\"].isna().sum()\n",
    "if missing_city:\n",
    "    print(f\"⚠️ Missing CityID for {missing_city} rows\")\n",
    "    fact_airport_totals = fact_airport_totals.dropna(subset=[\"CityID\"])\n",
    "\n",
    "# --- Save\n",
    "fact_airport_totals.to_csv(\"../data/clean/fact_airport_totals.csv\", index=False)\n",
    "print(f\"✅ fact_airport_totals rows: {len(fact_airport_totals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7877b",
   "metadata": {},
   "source": [
    "fact_citypairs (from Table 5 long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd6c2a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ fact_citypairs rows: 17664\n",
      "⚠️ AUS_CityID missing (pre-drop): 0\n",
      "⚠️ FOR_CityID missing (pre-drop): 0\n",
      "⚠️ DateID missing: 0\n",
      "\n",
      "Top unmatched Australian_City values:\n",
      " Series([], )\n",
      "\n",
      "Top unmatched Foreign_City values:\n",
      " Series([], )\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- reload inputs (or reuse your existing variables) ---\n",
    "t5_long = pd.read_csv(\"../data/clean/Table5_pairs_long.csv\")  # Australian_City, Foreign_City, Metric, Direction, Year, Value\n",
    "dim_citycountry = pd.read_csv(\"../data/clean/Dim_CityCountry.csv\")  # CityID, City, Country\n",
    "dim_date = pd.read_csv(\"../data/clean/dim_date.csv\")  # DateID, Year, ...\n",
    "\n",
    "# --- normalise ---\n",
    "for c in [\"Australian_City\",\"Foreign_City\"]:\n",
    "    t5_long[c] = (t5_long[c].astype(str)\n",
    "                              .str.strip()\n",
    "                              .str.replace(r\"\\s+\", \" \", regex=True))\n",
    "\n",
    "dim_citycountry[\"City\"]    = dim_citycountry[\"City\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "dim_citycountry[\"Country\"] = dim_citycountry[\"Country\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "# --- 0) Drop aggregates & subtotal labels ---\n",
    "# rows where the “city” column is actually country-level or a subtotal\n",
    "def is_aggregate(label: str) -> bool:\n",
    "    s = str(label).strip()\n",
    "    if s.lower() == \"australia\":        # country total\n",
    "        return True\n",
    "    if s.lower().startswith(\"total\"):    # subtotal labels like \"Total, Broome\"\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "mask_bad = (\n",
    "    t5_long[\"Australian_City\"].apply(is_aggregate) |\n",
    "    t5_long[\"Foreign_City\"].apply(is_aggregate)\n",
    ")\n",
    "t5_long = t5_long[~mask_bad].copy()\n",
    "\n",
    "# --- 1) types ---\n",
    "t5_long[\"Year\"]  = pd.to_numeric(t5_long[\"Year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "t5_long[\"Value\"] = pd.to_numeric(t5_long[\"Value\"], errors=\"coerce\")\n",
    "\n",
    "# --- 2) Countries to enable (City,Country) joins ---\n",
    "t5_long[\"Australian_Country\"] = \"Australia\"\n",
    "\n",
    "city_to_country = (dim_citycountry\n",
    "                   .drop_duplicates(subset=[\"City\"])\n",
    "                   .set_index(\"City\")[\"Country\"]\n",
    "                   .to_dict())\n",
    "t5_long[\"Foreign_Country\"] = t5_long[\"Foreign_City\"].map(city_to_country)\n",
    "\n",
    "# Optional: quick alias fixes if any spelling mismatches show up later\n",
    "alias_map = {\n",
    "    # \"Ho Chi Minh\": \"Ho Chi Minh City\",\n",
    "    # \"Xi’an\": \"Xi'an\",\n",
    "}\n",
    "for col in [\"Australian_City\",\"Foreign_City\"]:\n",
    "    t5_long[col] = t5_long[col].replace(alias_map)\n",
    "\n",
    "# --- 3) Merge AUS side (City, Country) -> AUS_CityID ---\n",
    "f5 = t5_long.merge(\n",
    "    dim_citycountry.rename(columns={\"City\":\"Australian_City\", \"Country\":\"Australian_Country\", \"CityID\":\"AUS_CityID\"})[\n",
    "        [\"AUS_CityID\",\"Australian_City\",\"Australian_Country\"]\n",
    "    ],\n",
    "    on=[\"Australian_City\",\"Australian_Country\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# --- 4) Merge foreign side (City, Country) -> FOR_CityID ---\n",
    "f5 = f5.merge(\n",
    "    dim_citycountry.rename(columns={\"City\":\"Foreign_City\", \"Country\":\"Foreign_Country\", \"CityID\":\"FOR_CityID\"})[\n",
    "        [\"FOR_CityID\",\"Foreign_City\",\"Foreign_Country\"]\n",
    "    ],\n",
    "    on=[\"Foreign_City\",\"Foreign_Country\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# --- 5) Year-based DateID merge ---\n",
    "dim_date[\"Year\"] = pd.to_numeric(dim_date[\"Year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "f5 = f5.merge(dim_date[[\"DateID\",\"Year\"]], on=\"Year\", how=\"left\")\n",
    "\n",
    "# --- 6) Final fact (only keep rows where both city IDs exist) ---\n",
    "fact_citypairs = (f5\n",
    "    .dropna(subset=[\"AUS_CityID\",\"FOR_CityID\"])\n",
    "    .rename(columns={\"Value\":\"Amount\"})[\n",
    "        [\"DateID\",\"AUS_CityID\",\"FOR_CityID\",\"Metric\",\"Direction\",\"Amount\"]\n",
    "    ])\n",
    "\n",
    "fact_citypairs.to_csv(\"../data/clean/fact_citypairs.csv\", index=False)\n",
    "\n",
    "# --- 7) Diagnostics ---\n",
    "print(\"✅ fact_citypairs rows:\", len(fact_citypairs))\n",
    "print(\"⚠️ AUS_CityID missing (pre-drop):\", f5[\"AUS_CityID\"].isna().sum())\n",
    "print(\"⚠️ FOR_CityID missing (pre-drop):\", f5[\"FOR_CityID\"].isna().sum())\n",
    "print(\"⚠️ DateID missing:\", fact_citypairs[\"DateID\"].isna().sum())\n",
    "\n",
    "# Show top unmatched AUS/Foreign city labels to build alias_map if needed\n",
    "unmatched_aus = (f5[f5[\"AUS_CityID\"].isna()]\n",
    "                 .groupby(\"Australian_City\").size()\n",
    "                 .sort_values(ascending=False).head(20))\n",
    "unmatched_for = (f5[f5[\"FOR_CityID\"].isna()]\n",
    "                 .groupby(\"Foreign_City\").size()\n",
    "                 .sort_values(ascending=False).head(20))\n",
    "print(\"\\nTop unmatched Australian_City values:\\n\", unmatched_aus.to_string())\n",
    "print(\"\\nTop unmatched Foreign_City values:\\n\", unmatched_for.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1a37e",
   "metadata": {},
   "source": [
    "fact market share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ae469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83fefacc",
   "metadata": {},
   "source": [
    "Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98f51ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dim_airline.csv                          rows=97 cols=3\n",
      "Loaded dim_citycountry.csv                      rows=84 cols=3\n",
      "Loaded dim_date.csv                             rows=204 cols=6\n",
      "Loaded fact_airline_monthly.csv                 rows=238 cols=7\n",
      "Loaded fact_airline_flights.csv                 rows=222 cols=8\n",
      "Loaded fact_airport_totals.csv                  rows=144 cols=6\n",
      "Loaded fact_citypairs.csv                       rows=17664 cols=6\n",
      "\n",
      "============ [dim_airline] required columns ============\n",
      "Missing: []\n",
      "Extra  : ['Home_Country']\n",
      "\n",
      "============ [dim_citycountry] required columns ============\n",
      "Missing: []\n",
      "Extra  : None\n",
      "\n",
      "============ [dim_date] required columns ============\n",
      "Missing: []\n",
      "Extra  : ['YearMonthDate', 'MonthName', 'Quarter']\n",
      "\n",
      "============ [fact_airline_monthly] required columns ============\n",
      "Missing: []\n",
      "Extra  : None\n",
      "\n",
      "============ [fact_airline_flights] required columns ============\n",
      "Missing: []\n",
      "Extra  : None\n",
      "\n",
      "============ [fact_airport_totals] required columns ============\n",
      "Missing: []\n",
      "Extra  : ['City']\n",
      "\n",
      "============ [fact_citypairs] required columns ============\n",
      "Missing: []\n",
      "Extra  : None\n",
      "\n",
      "============ [dim_airline] unique key ['AirlineID'] ============\n",
      "Duplicate key rows: 0\n",
      "\n",
      "============ [dim_citycountry] unique key ['CityID'] ============\n",
      "Duplicate key rows: 0\n",
      "\n",
      "============ [dim_date] unique key ['DateID'] ============\n",
      "Duplicate key rows: 0\n",
      "\n",
      "============ [dim_airline] nulls in ['AirlineID', 'Airline_Name'] ============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AirlineID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Airline_Name</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              null_count\n",
       "AirlineID              0\n",
       "Airline_Name           0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ [dim_citycountry] nulls in ['CityID', 'City', 'Country'] ============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CityID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         null_count\n",
       "CityID            0\n",
       "City              0\n",
       "Country           0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ [dim_date] nulls in ['DateID', 'Year', 'Month'] ============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DateID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        null_count\n",
       "DateID           0\n",
       "Year             0\n",
       "Month            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ [fact_airline_monthly] nulls in ['DateID', 'AirlineID', 'Direction', 'Passengers'] ============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Passengers</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AirlineID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Direction</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            null_count\n",
       "Passengers          29\n",
       "DateID               0\n",
       "AirlineID            0\n",
       "Direction            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ [fact_airline_flights] nulls in ['DateID', 'AirlineID', 'Direction', 'Flights', 'Seats_Available'] ============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Seats_Available</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flights</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AirlineID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Direction</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 null_count\n",
       "Seats_Available          28\n",
       "Flights                   7\n",
       "DateID                    0\n",
       "AirlineID                 0\n",
       "Direction                 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ [fact_airport_totals] nulls in ['DateID', 'CityID', 'Metric', 'Direction', 'Amount'] ============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amount</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CityID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Direction</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           null_count\n",
       "Amount             21\n",
       "DateID              0\n",
       "CityID              0\n",
       "Metric              0\n",
       "Direction           0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ [fact_citypairs] nulls in ['DateID', 'AUS_CityID', 'FOR_CityID', 'Metric', 'Direction', 'Amount'] ============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DateID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUS_CityID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOR_CityID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Direction</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amount</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            null_count\n",
       "DateID               0\n",
       "AUS_CityID           0\n",
       "FOR_CityID           0\n",
       "Metric               0\n",
       "Direction            0\n",
       "Amount               0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ [fact_airline_monthly] FK → dim_date ============\n",
      "DateID: missing keys = 0\n",
      "\n",
      "============ [fact_airline_monthly] FK → dim_airline ============\n",
      "AirlineID: missing keys = 0\n",
      "\n",
      "============ [fact_airline_flights] FK → dim_date ============\n",
      "DateID: missing keys = 0\n",
      "\n",
      "============ [fact_airline_flights] FK → dim_airline ============\n",
      "AirlineID: missing keys = 0\n",
      "\n",
      "============ [fact_airport_totals] FK → dim_date ============\n",
      "DateID: missing keys = 0\n",
      "\n",
      "============ [fact_airport_totals] FK → dim_citycountry ============\n",
      "CityID: missing keys = 0\n",
      "\n",
      "============ [fact_citypairs] FK → dim_date ============\n",
      "DateID: missing keys = 0\n",
      "\n",
      "============ [fact_citypairs] FK → dim_citycountry ============\n",
      "AUS_CityID: missing keys = 0\n",
      "\n",
      "============ [fact_citypairs] FK → dim_citycountry ============\n",
      "FOR_CityID: missing keys = 0\n",
      "\n",
      "============ [fact_airline_monthly] non-negative checks ============\n",
      "Passengers: negatives = 0\n",
      "Freight_tonnes: negatives = 0\n",
      "Mail_tonnes: negatives = 0\n",
      "\n",
      "============ [fact_airline_flights] non-negative checks ============\n",
      "Flights: negatives = 0\n",
      "Passengers: negatives = 0\n",
      "Seats_Available: negatives = 0\n",
      "\n",
      "============ [fact_airport_totals] non-negative checks ============\n",
      "Amount: negatives = 0\n",
      "\n",
      "============ [fact_citypairs] non-negative checks ============\n",
      "Amount: negatives = 0\n",
      "\n",
      "============ [fact_airline_flights] seat utilisation 0..1 ============\n",
      "Seat_Utilisation outside [0,1]: 0\n",
      "\n",
      "============ Reconciliation snapshots ============\n",
      "Latest DateID: 202512 2025-12\n",
      "Airline-level pax reconciliation (latest month) — top 10 abs diff:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_aed9f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_aed9f_level0_col0\" class=\"col_heading level0 col0\" >pax_f1</th>\n",
       "      <th id=\"T_aed9f_level0_col1\" class=\"col_heading level0 col1\" >pax_f3</th>\n",
       "      <th id=\"T_aed9f_level0_col2\" class=\"col_heading level0 col2\" >diff</th>\n",
       "      <th id=\"T_aed9f_level0_col3\" class=\"col_heading level0 col3\" >abs_diff_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x15b2201c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ City totals reconciliation — Inbound ============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_9c04b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_9c04b_level0_col0\" class=\"col_heading level0 col0\" >DateID</th>\n",
       "      <th id=\"T_9c04b_level0_col1\" class=\"col_heading level0 col1\" >AUS_CityID</th>\n",
       "      <th id=\"T_9c04b_level0_col2\" class=\"col_heading level0 col2\" >cp_in</th>\n",
       "      <th id=\"T_9c04b_level0_col3\" class=\"col_heading level0 col3\" >CityID</th>\n",
       "      <th id=\"T_9c04b_level0_col4\" class=\"col_heading level0 col4\" >at_in</th>\n",
       "      <th id=\"T_9c04b_level0_col5\" class=\"col_heading level0 col5\" >diff</th>\n",
       "      <th id=\"T_9c04b_level0_col6\" class=\"col_heading level0 col6\" >abs_diff_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_9c04b_row0_col0\" class=\"data row0 col0\" >202401</td>\n",
       "      <td id=\"T_9c04b_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_9c04b_row0_col2\" class=\"data row0 col2\" >35227.000000</td>\n",
       "      <td id=\"T_9c04b_row0_col3\" class=\"data row0 col3\" >1</td>\n",
       "      <td id=\"T_9c04b_row0_col4\" class=\"data row0 col4\" >35227.000000</td>\n",
       "      <td id=\"T_9c04b_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
       "      <td id=\"T_9c04b_row0_col6\" class=\"data row0 col6\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9c04b_row1_col0\" class=\"data row1 col0\" >202401</td>\n",
       "      <td id=\"T_9c04b_row1_col1\" class=\"data row1 col1\" >2</td>\n",
       "      <td id=\"T_9c04b_row1_col2\" class=\"data row1 col2\" >221626.000000</td>\n",
       "      <td id=\"T_9c04b_row1_col3\" class=\"data row1 col3\" >2</td>\n",
       "      <td id=\"T_9c04b_row1_col4\" class=\"data row1 col4\" >221626.000000</td>\n",
       "      <td id=\"T_9c04b_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n",
       "      <td id=\"T_9c04b_row1_col6\" class=\"data row1 col6\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9c04b_row2_col0\" class=\"data row2 col0\" >202501</td>\n",
       "      <td id=\"T_9c04b_row2_col1\" class=\"data row2 col1\" >10</td>\n",
       "      <td id=\"T_9c04b_row2_col2\" class=\"data row2 col2\" >1576.000000</td>\n",
       "      <td id=\"T_9c04b_row2_col3\" class=\"data row2 col3\" >10</td>\n",
       "      <td id=\"T_9c04b_row2_col4\" class=\"data row2 col4\" >1576.000000</td>\n",
       "      <td id=\"T_9c04b_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n",
       "      <td id=\"T_9c04b_row2_col6\" class=\"data row2 col6\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9c04b_row3_col0\" class=\"data row3 col0\" >202501</td>\n",
       "      <td id=\"T_9c04b_row3_col1\" class=\"data row3 col1\" >9</td>\n",
       "      <td id=\"T_9c04b_row3_col2\" class=\"data row3 col2\" >201195.000000</td>\n",
       "      <td id=\"T_9c04b_row3_col3\" class=\"data row3 col3\" >9</td>\n",
       "      <td id=\"T_9c04b_row3_col4\" class=\"data row3 col4\" >201195.000000</td>\n",
       "      <td id=\"T_9c04b_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n",
       "      <td id=\"T_9c04b_row3_col6\" class=\"data row3 col6\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9c04b_row4_col0\" class=\"data row4 col0\" >202501</td>\n",
       "      <td id=\"T_9c04b_row4_col1\" class=\"data row4 col1\" >8</td>\n",
       "      <td id=\"T_9c04b_row4_col2\" class=\"data row4 col2\" >449863.000000</td>\n",
       "      <td id=\"T_9c04b_row4_col3\" class=\"data row4 col3\" >8</td>\n",
       "      <td id=\"T_9c04b_row4_col4\" class=\"data row4 col4\" >449863.000000</td>\n",
       "      <td id=\"T_9c04b_row4_col5\" class=\"data row4 col5\" >0.000000</td>\n",
       "      <td id=\"T_9c04b_row4_col6\" class=\"data row4 col6\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14f21a0e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ City totals reconciliation — Outbound ============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_dfd79\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_dfd79_level0_col0\" class=\"col_heading level0 col0\" >DateID</th>\n",
       "      <th id=\"T_dfd79_level0_col1\" class=\"col_heading level0 col1\" >AUS_CityID</th>\n",
       "      <th id=\"T_dfd79_level0_col2\" class=\"col_heading level0 col2\" >cp_out</th>\n",
       "      <th id=\"T_dfd79_level0_col3\" class=\"col_heading level0 col3\" >CityID</th>\n",
       "      <th id=\"T_dfd79_level0_col4\" class=\"col_heading level0 col4\" >at_out</th>\n",
       "      <th id=\"T_dfd79_level0_col5\" class=\"col_heading level0 col5\" >diff</th>\n",
       "      <th id=\"T_dfd79_level0_col6\" class=\"col_heading level0 col6\" >abs_diff_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_dfd79_row0_col0\" class=\"data row0 col0\" >202401</td>\n",
       "      <td id=\"T_dfd79_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_dfd79_row0_col2\" class=\"data row0 col2\" >37267.000000</td>\n",
       "      <td id=\"T_dfd79_row0_col3\" class=\"data row0 col3\" >1</td>\n",
       "      <td id=\"T_dfd79_row0_col4\" class=\"data row0 col4\" >37267.000000</td>\n",
       "      <td id=\"T_dfd79_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
       "      <td id=\"T_dfd79_row0_col6\" class=\"data row0 col6\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dfd79_row1_col0\" class=\"data row1 col0\" >202401</td>\n",
       "      <td id=\"T_dfd79_row1_col1\" class=\"data row1 col1\" >2</td>\n",
       "      <td id=\"T_dfd79_row1_col2\" class=\"data row1 col2\" >231012.000000</td>\n",
       "      <td id=\"T_dfd79_row1_col3\" class=\"data row1 col3\" >2</td>\n",
       "      <td id=\"T_dfd79_row1_col4\" class=\"data row1 col4\" >231012.000000</td>\n",
       "      <td id=\"T_dfd79_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n",
       "      <td id=\"T_dfd79_row1_col6\" class=\"data row1 col6\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dfd79_row2_col0\" class=\"data row2 col0\" >202501</td>\n",
       "      <td id=\"T_dfd79_row2_col1\" class=\"data row2 col1\" >10</td>\n",
       "      <td id=\"T_dfd79_row2_col2\" class=\"data row2 col2\" >1507.000000</td>\n",
       "      <td id=\"T_dfd79_row2_col3\" class=\"data row2 col3\" >10</td>\n",
       "      <td id=\"T_dfd79_row2_col4\" class=\"data row2 col4\" >1507.000000</td>\n",
       "      <td id=\"T_dfd79_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n",
       "      <td id=\"T_dfd79_row2_col6\" class=\"data row2 col6\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dfd79_row3_col0\" class=\"data row3 col0\" >202501</td>\n",
       "      <td id=\"T_dfd79_row3_col1\" class=\"data row3 col1\" >9</td>\n",
       "      <td id=\"T_dfd79_row3_col2\" class=\"data row3 col2\" >206834.000000</td>\n",
       "      <td id=\"T_dfd79_row3_col3\" class=\"data row3 col3\" >9</td>\n",
       "      <td id=\"T_dfd79_row3_col4\" class=\"data row3 col4\" >206834.000000</td>\n",
       "      <td id=\"T_dfd79_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n",
       "      <td id=\"T_dfd79_row3_col6\" class=\"data row3 col6\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dfd79_row4_col0\" class=\"data row4 col0\" >202501</td>\n",
       "      <td id=\"T_dfd79_row4_col1\" class=\"data row4 col1\" >8</td>\n",
       "      <td id=\"T_dfd79_row4_col2\" class=\"data row4 col2\" >460578.000000</td>\n",
       "      <td id=\"T_dfd79_row4_col3\" class=\"data row4 col3\" >8</td>\n",
       "      <td id=\"T_dfd79_row4_col4\" class=\"data row4 col4\" >460578.000000</td>\n",
       "      <td id=\"T_dfd79_row4_col5\" class=\"data row4 col5\" >0.000000</td>\n",
       "      <td id=\"T_dfd79_row4_col6\" class=\"data row4 col6\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14f206140>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Sanity checks complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Sanity checks for cleaned aviation model ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BASE = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "CLEAN = BASE / \"data\" / \"clean\"\n",
    "\n",
    "# ---------- Load ----------\n",
    "def load_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"Loaded {path.name:40s} rows={len(df)} cols={len(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "dim_airline        = load_csv(CLEAN / \"dim_airline.csv\")              # AirlineID, Airline_Name\n",
    "dim_citycountry    = load_csv(CLEAN / \"dim_citycountry.csv\")          # CityID, City, Country\n",
    "dim_date           = load_csv(CLEAN / \"dim_date.csv\")                 # DateID, Year, Month, YearMonthDate\n",
    "\n",
    "fact_airline_monthly = load_csv(CLEAN / \"fact_airline_monthly.csv\")   # DateID, AirlineID, Country, Direction, Passengers, Freight_tonnes, Mail_tonnes\n",
    "fact_airline_flights = load_csv(CLEAN / \"fact_airline_flights.csv\")   # DateID, AirlineID, Service_Region, Direction, Flights, Passengers, Seats_Available, Seat_Utilisation\n",
    "fact_airport_totals  = load_csv(CLEAN / \"fact_airport_totals.csv\")    # DateID, CityID, Metric, Direction, Amount\n",
    "fact_citypairs       = load_csv(CLEAN / \"fact_citypairs.csv\")         # DateID, AUS_CityID, FOR_CityID, Metric, Direction, Amount\n",
    "\n",
    "# ---------- Helper reporting ----------\n",
    "def header(t): print(f\"\\n{'='*12} {t} {'='*12}\")\n",
    "def show(df, n=5): \n",
    "    display(df.head(n).style.hide(axis='index'))\n",
    "\n",
    "def check_columns(df, required, name):\n",
    "    header(f\"[{name}] required columns\")\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    extra   = [c for c in df.columns if c not in required]\n",
    "    print(\"Missing:\", missing)\n",
    "    print(\"Extra  :\", extra if extra else \"None\")\n",
    "\n",
    "def check_unique(df, keys, name):\n",
    "    header(f\"[{name}] unique key {keys}\")\n",
    "    dups = df.duplicated(subset=keys, keep=False)\n",
    "    print(\"Duplicate key rows:\", dups.sum())\n",
    "    if dups.any(): show(df.loc[dups].head(10))\n",
    "\n",
    "def check_nulls(df, cols, name):\n",
    "    header(f\"[{name}] nulls in {cols}\")\n",
    "    nulls = df[cols].isna().sum().sort_values(ascending=False)\n",
    "    display(nulls.to_frame(\"null_count\"))\n",
    "\n",
    "def check_fk(fact, fact_cols, dim, dim_key, name, dim_name):\n",
    "    \"\"\"fact_cols: list of columns in fact that must exist in dim[dim_key]\"\"\"\n",
    "    header(f\"[{name}] FK → {dim_name}\")\n",
    "    dim_keys = set(dim[dim_key].dropna().unique())\n",
    "    for col in fact_cols:\n",
    "        missing = ~fact[col].isin(dim_keys)\n",
    "        cnt = missing.sum()\n",
    "        print(f\"{col}: missing keys = {cnt}\")\n",
    "        if cnt:\n",
    "            show(fact.loc[missing, [col]].drop_duplicates().head(10))\n",
    "\n",
    "def check_nonnegative(df, cols, name):\n",
    "    header(f\"[{name}] non-negative checks\")\n",
    "    for c in cols:\n",
    "        if c in df:\n",
    "            bad = (df[c] < 0) & df[c].notna()\n",
    "            print(f\"{c}: negatives = {bad.sum()}\")\n",
    "            if bad.any(): show(df.loc[bad, [c]].head(10))\n",
    "\n",
    "# ---------- Expected schemas ----------\n",
    "cols_dim_airline = [\"AirlineID\",\"Airline_Name\"]\n",
    "cols_dim_city    = [\"CityID\",\"City\",\"Country\"]\n",
    "cols_dim_date    = [\"DateID\",\"Year\",\"Month\"]  # plus YearMonthDate allowed\n",
    "\n",
    "cols_f1 = [\"DateID\",\"AirlineID\",\"Country\",\"Direction\",\"Passengers\",\"Freight_tonnes\",\"Mail_tonnes\"]\n",
    "cols_f3 = [\"DateID\",\"AirlineID\",\"Service_Region\",\"Direction\",\"Flights\",\"Passengers\",\"Seats_Available\",\"Seat_Utilisation\"]\n",
    "cols_f4 = [\"DateID\",\"CityID\",\"Metric\",\"Direction\",\"Amount\"]\n",
    "cols_f5 = [\"DateID\",\"AUS_CityID\",\"FOR_CityID\",\"Metric\",\"Direction\",\"Amount\"]\n",
    "\n",
    "# ---------- Column checks ----------\n",
    "check_columns(dim_airline, cols_dim_airline, \"dim_airline\")\n",
    "check_columns(dim_citycountry, cols_dim_city, \"dim_citycountry\")\n",
    "check_columns(dim_date, cols_dim_date, \"dim_date\")\n",
    "\n",
    "check_columns(fact_airline_monthly, cols_f1, \"fact_airline_monthly\")\n",
    "check_columns(fact_airline_flights, cols_f3, \"fact_airline_flights\")\n",
    "check_columns(fact_airport_totals, cols_f4, \"fact_airport_totals\")\n",
    "check_columns(fact_citypairs, cols_f5, \"fact_citypairs\")\n",
    "\n",
    "# ---------- Key uniqueness ----------\n",
    "check_unique(dim_airline, [\"AirlineID\"], \"dim_airline\")\n",
    "check_unique(dim_citycountry, [\"CityID\"], \"dim_citycountry\")\n",
    "check_unique(dim_date, [\"DateID\"], \"dim_date\")\n",
    "\n",
    "# ---------- Nulls on key columns ----------\n",
    "check_nulls(dim_airline, [\"AirlineID\",\"Airline_Name\"], \"dim_airline\")\n",
    "check_nulls(dim_citycountry, [\"CityID\",\"City\",\"Country\"], \"dim_citycountry\")\n",
    "check_nulls(dim_date, [\"DateID\",\"Year\",\"Month\"], \"dim_date\")\n",
    "\n",
    "check_nulls(fact_airline_monthly, [\"DateID\",\"AirlineID\",\"Direction\",\"Passengers\"], \"fact_airline_monthly\")\n",
    "check_nulls(fact_airline_flights, [\"DateID\",\"AirlineID\",\"Direction\",\"Flights\",\"Seats_Available\"], \"fact_airline_flights\")\n",
    "check_nulls(fact_airport_totals, [\"DateID\",\"CityID\",\"Metric\",\"Direction\",\"Amount\"], \"fact_airport_totals\")\n",
    "check_nulls(fact_citypairs, [\"DateID\",\"AUS_CityID\",\"FOR_CityID\",\"Metric\",\"Direction\",\"Amount\"], \"fact_citypairs\")\n",
    "\n",
    "# ---------- FK integrity ----------\n",
    "check_fk(fact_airline_monthly, [\"DateID\"], dim_date, \"DateID\", \"fact_airline_monthly\", \"dim_date\")\n",
    "check_fk(fact_airline_monthly, [\"AirlineID\"], dim_airline, \"AirlineID\", \"fact_airline_monthly\", \"dim_airline\")\n",
    "\n",
    "check_fk(fact_airline_flights, [\"DateID\"], dim_date, \"DateID\", \"fact_airline_flights\", \"dim_date\")\n",
    "check_fk(fact_airline_flights, [\"AirlineID\"], dim_airline, \"AirlineID\", \"fact_airline_flights\", \"dim_airline\")\n",
    "\n",
    "check_fk(fact_airport_totals, [\"DateID\"], dim_date, \"DateID\", \"fact_airport_totals\", \"dim_date\")\n",
    "check_fk(fact_airport_totals, [\"CityID\"], dim_citycountry, \"CityID\", \"fact_airport_totals\", \"dim_citycountry\")\n",
    "\n",
    "check_fk(fact_citypairs, [\"DateID\"], dim_date, \"DateID\", \"fact_citypairs\", \"dim_date\")\n",
    "check_fk(fact_citypairs, [\"AUS_CityID\"], dim_citycountry, \"CityID\", \"fact_citypairs\", \"dim_citycountry\")\n",
    "check_fk(fact_citypairs, [\"FOR_CityID\"], dim_citycountry, \"CityID\", \"fact_citypairs\", \"dim_citycountry\")\n",
    "\n",
    "# ---------- Value/range checks ----------\n",
    "check_nonnegative(fact_airline_monthly, [\"Passengers\",\"Freight_tonnes\",\"Mail_tonnes\"], \"fact_airline_monthly\")\n",
    "check_nonnegative(fact_airline_flights, [\"Flights\",\"Passengers\",\"Seats_Available\"], \"fact_airline_flights\")\n",
    "check_nonnegative(fact_airport_totals, [\"Amount\"], \"fact_airport_totals\")\n",
    "check_nonnegative(fact_citypairs, [\"Amount\"], \"fact_citypairs\")\n",
    "\n",
    "header(\"[fact_airline_flights] seat utilisation 0..1\")\n",
    "if \"Seat_Utilisation\" in fact_airline_flights:\n",
    "    out_of_bounds = fact_airline_flights[\"Seat_Utilisation\"].dropna().pipe(lambda s: (s<0) | (s>1)).sum()\n",
    "    print(\"Seat_Utilisation outside [0,1]:\", out_of_bounds)\n",
    "    if out_of_bounds:\n",
    "        show(fact_airline_flights.loc[(fact_airline_flights[\"Seat_Utilisation\"]<0) | (fact_airline_flights[\"Seat_Utilisation\"]>1)].head(10))\n",
    "\n",
    "# ---------- Basic reconciliation snapshots ----------\n",
    "header(\"Reconciliation snapshots\")\n",
    "\n",
    "# 1) Passengers by airline in May-2025 across tables 1 & 3 (if both have May-2025)\n",
    "def label_for_dateid(did):\n",
    "    row = dim_date.loc[dim_date[\"DateID\"]==did]\n",
    "    if row.empty: return \"?\"\n",
    "    return f'{int(row[\"Year\"].values[0])}-{int(row[\"Month\"].values[0]):02d}'\n",
    "\n",
    "# Pick the latest DateID available\n",
    "latest_dateid = dim_date[\"DateID\"].max() if \"DateID\" in dim_date else None\n",
    "print(\"Latest DateID:\", latest_dateid, label_for_dateid(latest_dateid) if latest_dateid is not None else \"\")\n",
    "\n",
    "if latest_dateid is not None:\n",
    "    f1_latest = fact_airline_monthly.query(\"DateID == @latest_dateid\")\n",
    "    f3_latest = fact_airline_flights.query(\"DateID == @latest_dateid\")\n",
    "\n",
    "    pax_by_airline_f1 = (f1_latest.groupby(\"AirlineID\")[\"Passengers\"].sum().rename(\"pax_f1\"))\n",
    "    pax_by_airline_f3 = (f3_latest.groupby(\"AirlineID\")[\"Passengers\"].sum().rename(\"pax_f3\"))\n",
    "\n",
    "    recon = (pax_by_airline_f1.to_frame()\n",
    "             .merge(pax_by_airline_f3.to_frame(), left_index=True, right_index=True, how=\"outer\")\n",
    "             .fillna(0.0))\n",
    "    recon[\"diff\"] = recon[\"pax_f3\"] - recon[\"pax_f1\"]\n",
    "    recon[\"abs_diff_pct\"] = np.where(recon[\"pax_f1\"]>0, abs(recon[\"diff\"])/recon[\"pax_f1\"], np.nan)\n",
    "    print(\"Airline-level pax reconciliation (latest month) — top 10 abs diff:\")\n",
    "    show(recon.sort_values(\"abs_diff_pct\", ascending=False).head(10))\n",
    "\n",
    "# 2) City totals check: citypairs (sum of inbound/outbound passengers) vs airport totals (Passengers metric)\n",
    "if \"Metric\" in fact_citypairs.columns and \"Metric\" in fact_airport_totals.columns:\n",
    "    # passengers only\n",
    "    cp_pax = fact_citypairs.query('Metric == \"Passengers\"')\n",
    "    at_pax = fact_airport_totals.query('Metric == \"Passengers\"')\n",
    "\n",
    "    # inbound to an AUS city = sum over foreign origins where Direction == 'Inbound'\n",
    "    cp_in = cp_pax.query('Direction == \"Inbound\"').groupby([\"DateID\",\"AUS_CityID\"])[\"Amount\"].sum().rename(\"cp_in\")\n",
    "    at_in = at_pax.query('Direction == \"Inbound\"').groupby([\"DateID\",\"CityID\"])[\"Amount\"].sum().rename(\"at_in\")\n",
    "\n",
    "    # outbound from an AUS city\n",
    "    cp_out = cp_pax.query('Direction == \"Outbound\"').groupby([\"DateID\",\"AUS_CityID\"])[\"Amount\"].sum().rename(\"cp_out\")\n",
    "    at_out = at_pax.query('Direction == \"Outbound\"').groupby([\"DateID\",\"CityID\"])[\"Amount\"].sum().rename(\"at_out\")\n",
    "\n",
    "    rec_in  = (cp_in.reset_index()\n",
    "               .merge(at_in.reset_index(), left_on=[\"DateID\",\"AUS_CityID\"], right_on=[\"DateID\",\"CityID\"], how=\"inner\"))\n",
    "    rec_out = (cp_out.reset_index()\n",
    "               .merge(at_out.reset_index(), left_on=[\"DateID\",\"AUS_CityID\"], right_on=[\"DateID\",\"CityID\"], how=\"inner\"))\n",
    "\n",
    "    for label, rec, left, right in [\n",
    "        (\"Inbound\", rec_in,  \"cp_in\",  \"at_in\"),\n",
    "        (\"Outbound\", rec_out,\"cp_out\", \"at_out\"),\n",
    "    ]:\n",
    "        header(f\"City totals reconciliation — {label}\")\n",
    "        if not rec.empty:\n",
    "            rec[\"diff\"] = rec[left] - rec[right]\n",
    "            rec[\"abs_diff_pct\"] = np.where(rec[right]>0, abs(rec[\"diff\"])/rec[right], np.nan)\n",
    "            show(rec.sort_values(\"abs_diff_pct\", ascending=False).head(10))\n",
    "        else:\n",
    "            print(\"No overlap to reconcile (check Metric/Direction labels).\")\n",
    "\n",
    "print(\"\\n✅ Sanity checks complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46c44f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
